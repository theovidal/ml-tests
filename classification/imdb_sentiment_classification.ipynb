{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# IMDB Sentiment analysis",
   "id": "e110e26ea4db6122"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-17T08:49:46.809319Z",
     "start_time": "2024-08-17T08:49:38.911685Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import random\n",
    "import time\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ],
   "id": "fbae34937d9c513c",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-17 10:49:39.959631: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-08-17 10:49:40.111534: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-08-17 10:49:40.152941: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-08-17 10:49:40.272660: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-08-17 10:49:42.945448: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Loading data",
   "id": "aeca41a2c62071f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-17T08:49:59.107710Z",
     "start_time": "2024-08-17T08:49:46.810999Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def separate_text_and_label(data_path):\n",
    "    \"\"\"\n",
    "    Read all the data and parse it (remove HTML and lowercase)\n",
    "\n",
    "    :param data_path:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    texts = []\n",
    "    labels = []\n",
    "    classes = {}\n",
    "    for category in ['pos', 'neg']:\n",
    "        classes[category] = []\n",
    "        full_path = os.path.join(data_path, category)\n",
    "        for filename in sorted(os.listdir(full_path)):\n",
    "            if filename.endswith('.txt'):\n",
    "                with open(os.path.join(full_path, filename)) as file:\n",
    "                    text = file.read().lower().replace('<br />', '')\n",
    "                    texts.append(text)\n",
    "                    classes[category].append(text)\n",
    "                labels.append(0 if category == 'neg' else 1)\n",
    "\n",
    "    return texts, labels, classes\n",
    "\n",
    "\n",
    "train_texts, train_labels, train_classes = separate_text_and_label('aclImdb/train')\n",
    "test_texts, test_labels, test_classes = separate_text_and_label('aclImdb/test')\n",
    "\n",
    "timestamp = int(time.time())\n",
    "random.Random(timestamp).shuffle(train_texts)\n",
    "random.Random(timestamp).shuffle(train_labels)"
   ],
   "id": "54365d1ce16b6b7e",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Analyzing the dataset\n",
    "\n",
    "- Number of samples\n",
    "- Number of classes,\n",
    "- Number of samples per class\n",
    "- Average number of words per sample,\n",
    "- Distribution of words per category and globally,\n",
    "- Distribution of the number of words per category and globally."
   ],
   "id": "206bbc8e450db983"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-17T08:49:59.115256Z",
     "start_time": "2024-08-17T08:49:59.110323Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_words(text):\n",
    "    \"\"\"\n",
    "    Get all the words in text\n",
    "    :param text: the text to extract words from\n",
    "    :return: list of words\n",
    "    \"\"\"\n",
    "    return text.split()\n",
    "\n",
    "\n",
    "def number_of_words_from_text(text):\n",
    "    \"\"\"\n",
    "    Get the number of words in text\n",
    "    :param text: the text to extract words from\n",
    "    :return: the number of words\n",
    "    \"\"\"\n",
    "    return len(get_words(text))\n",
    "\n",
    "\n",
    "def flatten_comprehension(list_of_lists):\n",
    "    \"\"\"\n",
    "    Flatten a list of lists\n",
    "    :param list_of_lists: a list of lists\n",
    "    :return: a flattened list of lists\n",
    "    \"\"\"\n",
    "    return [item for row in list_of_lists for item in row]"
   ],
   "id": "40821ebc63736669",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-17T08:50:00.431665Z",
     "start_time": "2024-08-17T08:49:59.117357Z"
    }
   },
   "cell_type": "code",
   "source": [
    "nb_samples = len(train_texts)\n",
    "classes_list, class_indexes, nb_samples_per_class = np.unique(train_labels, return_index=True, return_counts=True)\n",
    "nb_classes = len(classes_list)\n",
    "\n",
    "# np.vectorize maps every element of the array with a specific function\n",
    "word_counts = np.vectorize(number_of_words_from_text)(train_texts)\n",
    "# Thanks to this trick, we just have to calculate a mean of all the values\n",
    "avg_nb_words = np.mean(word_counts)"
   ],
   "id": "2c199d6b1442b1a8",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from collections import Counter\n",
    "\n",
    "\n",
    "def plot_word_distribution(texts, category):\n",
    "    \"\"\"\n",
    "    Plot the distribution of words and length across samples of text\n",
    "    :param texts: a list of texts\n",
    "    :param category: the category of text\n",
    "    \"\"\"\n",
    "    split = list(map(get_words, texts))\n",
    "\n",
    "    all_words = np.array(flatten_comprehension(split))\n",
    "    # Count the frequency of each word\n",
    "    word_counts = Counter(all_words)\n",
    "    # Extract words and their frequencies\n",
    "    words, frequencies = zip(*word_counts.most_common(30))\n",
    "\n",
    "    plt.figure(figsize=(12, 8))  # Increase figure size if needed\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.bar(words, frequencies)\n",
    "    plt.title(f'Frequency distribution of words for {category} samples')\n",
    "    plt.xlabel('Words')\n",
    "    plt.ylabel('Frequency in all texts')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()  # Adjust layout to fit labels\n",
    "    plt.xlim(0, 40)\n",
    "\n",
    "    nb_words_distribution = np.fromiter(map(lambda wds: len(wds), split), dtype=int)\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.hist(nb_words_distribution, 40)\n",
    "    plt.xlabel('Number of words')\n",
    "    plt.ylabel('Number of samples')\n",
    "    plt.title(f'Distribution of number of words for {category} samples')\n",
    "    \n",
    "\n",
    "print(f'Number of samples: {nb_samples}\\nNumber of classes_list: {nb_classes}\\nNumber of samples per class:')\n",
    "for i in range(len(classes_list)):\n",
    "      print(f'- {classes_list[i]}: {nb_samples_per_class[i]}')\n",
    "\n",
    "print(f'Average number of words per sample: {avg_nb_words}')\n",
    "\n",
    "for category in train_classes.keys():\n",
    "    plot_word_distribution(train_classes[category], category)\n",
    "plot_word_distribution(train_texts, 'all')"
   ],
   "id": "6f3490fb6c215b25",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Data pipeline",
   "id": "a70ba52525509c43"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### N-Grams data structure",
   "id": "5545b14c98750894"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif\n",
    "\n",
    "NUM_FEATURES = 20_000\n",
    "NGRAM_RANGE = (1, 2)\n",
    "\n",
    "def vectorize_ngrams(train_texts, train_labels, validation_texts):\n",
    "    # Step 1 - Feature extraction : we want to extract n-gram frequencies from our texts\n",
    "    vectorizer = TfidfVectorizer(\n",
    "        # For more security, we allow 1-grams to be extracted (if the sentence only contains one word for instance)\n",
    "        ngram_range=NGRAM_RANGE,\n",
    "        # In case accents are present (for instance, if the comments are from other languages than English)\n",
    "        # Here we know our data is in English, but this parameter could be useful in the future if we want to generalize\n",
    "        # ou rmodel\n",
    "        strip_accents='unicode',\n",
    "        # If other characters than unicode are detected, just replace them ; the essential is having a good vocabulary\n",
    "        # without repetitions\n",
    "        decode_error='replace',\n",
    "        # 'max_features': 20_000, # Will be treated by a feature selection process later\n",
    "\n",
    "        # Integer are simpler to compute, and we don't need floats for the moment (we are only processing the data here,\n",
    "        # not interacting with the coefficients)\n",
    "        dtype=np.float64,\n",
    "        # 'norm': None,\n",
    "\n",
    "        # Remove tokens that shows only once (only keep those which count is more than 2)\n",
    "        min_df=2\n",
    "    )\n",
    "\n",
    "    # This operation consists of:\n",
    "    # - Fitting: read texts, learn the vocabulary and calculate frequencies\n",
    "    # - Transforming: parse this data into a matrix\n",
    "    # The returned value is a matrix of shape (n_samples, n_features) : we can get the number of extracted features\n",
    "    # right from it. It will then be used for our MLP model\n",
    "    x_train = vectorizer.fit_transform(train_texts)\n",
    "    # Here we don't need to learn the vocabulary, this was done at the precedent line\n",
    "    # (and it could cause problems if some words are different)\n",
    "    x_validation = vectorizer.transform(validation_texts)\n",
    "\n",
    "    # Step 2 - Feature selection : we only consider the features that are relevant for our purpose We choose the\n",
    "    # classification function as we face a classification problem\n",
    "\n",
    "    # Note the difference between the vectorizer - to return a vector out of another object - and the transformer\n",
    "\n",
    "    # BE CAREFUL: if the dataset has less than 20k\n",
    "    # features, we need a fallback -> the number of features already present\n",
    "    selector = SelectKBest(score_func=f_classif, k=min(NUM_FEATURES, x_train.shape[1]))\n",
    "    selector.fit(x_train, train_labels)\n",
    "\n",
    "    x_train = selector.transform(x_train).astype(np.float64)\n",
    "    x_validation = selector.transform(x_validation).astype(np.float64)\n",
    "\n",
    "    return x_train, x_validation"
   ],
   "id": "24f75f499602597f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Creating the vocabulary and the vectorization preprocessing",
   "id": "20aa75195a9e3d47"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-17T08:50:09.277329Z",
     "start_time": "2024-08-17T08:50:00.433426Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "num_features = 20_000\n",
    "max_sequence_length = 500\n",
    "validation_split = 0.2\n",
    "\n",
    "count = CountVectorizer(\n",
    "  lowercase=True,\n",
    "  min_df=2,\n",
    "  strip_accents='unicode',\n",
    "  max_features=num_features\n",
    ")\n",
    "\n",
    "count.fit(train_texts)\n",
    "\n",
    "vocabulary = list(count.vocabulary_.keys())\n",
    "vectorize_layer = tf.keras.layers.TextVectorization(\n",
    "  vocabulary=vocabulary,\n",
    "  output_mode='int',\n",
    "  output_sequence_length=max_sequence_length\n",
    ")\n",
    "\n",
    "def create_dataset(texts, labels):\n",
    "    # Create a unique dataset for the two parts, features and labels, then use\n",
    "    # a pre-processing pipeline which TensorFlow will optimize\n",
    "    return tf.data.Dataset.from_tensor_slices((texts, labels)).batch(32).map(lambda x, y: (vectorize_layer(x), y))"
   ],
   "id": "2e83661a324ee5a9",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1723884608.515898   36361 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1723884608.911237   36361 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1723884608.911910   36361 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1723884608.915200   36361 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1723884608.915866   36361 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1723884608.916414   36361 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1723884609.086359   36361 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1723884609.087006   36361 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1723884609.090764   36361 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-08-17 10:50:09.091653: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 1270 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1050, pci bus id: 0000:01:00.0, compute capability: 6.1\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-17T08:50:12.396851Z",
     "start_time": "2024-08-17T08:50:09.279664Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "dataset = create_dataset(train_texts, train_labels)\n",
    "# Be careful when splitting the dataset: .take() and .skip() count in batches, not number of samples\n",
    "dataset_size = sum(1 for _ in dataset)\n",
    "validation_size = int(validation_split * dataset_size)\n",
    "validation_dataset = dataset.take(validation_size)\n",
    "train_dataset = dataset.skip(validation_size)\n",
    "\n",
    "test_dataset = create_dataset(test_texts, test_labels)"
   ],
   "id": "db233d2a2e3d66ec",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-17 10:50:11.805293: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Getting a word embedding",
   "id": "df22e2308ac56799"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-17T08:50:16.326584Z",
     "start_time": "2024-08-17T08:50:12.398542Z"
    }
   },
   "cell_type": "code",
   "source": [
    "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
    "!unzip -q glove.6B.zip"
   ],
   "id": "86cf6ead9fcb3813",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-08-17 10:50:12--  http://nlp.stanford.edu/data/glove.6B.zip\r\n",
      "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\r\n",
      "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\r\n",
      "HTTP request sent, awaiting response... 302 Found\r\n",
      "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\r\n",
      "--2024-08-17 10:50:13--  https://nlp.stanford.edu/data/glove.6B.zip\r\n",
      "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\r\n",
      "HTTP request sent, awaiting response... 301 Moved Permanently\r\n",
      "Location: https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\r\n",
      "--2024-08-17 10:50:14--  https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\r\n",
      "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\r\n",
      "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\r\n",
      "HTTP request sent, awaiting response... ^C\r\n",
      "replace glove.6B.50d.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: ^C\r\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-17T08:50:47.620184Z",
     "start_time": "2024-08-17T08:50:21.839793Z"
    }
   },
   "cell_type": "code",
   "source": [
    "embedding_path = 'glove.6B.300d.txt'\n",
    "embeddings = {}\n",
    "\n",
    "with open(embedding_path, 'r') as f:\n",
    "    for line in f.readlines():\n",
    "        word, coefs = line.split(maxsplit=1)\n",
    "        vec = np.fromstring(coefs, 'f', sep=' ')\n",
    "        embeddings[word] = vec\n",
    "        \n",
    "embedding_dim = embeddings['the'].shape[0]\n",
    "\n",
    "embedding_matrix = np.zeros((num_features, embedding_dim))\n",
    "for i, word in enumerate(vocabulary):\n",
    "    vec = embeddings.get(word)\n",
    "    if vec is not None:\n",
    "        embedding_matrix[i] = vec"
   ],
   "id": "d2fc8197de51e057",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Creating various models",
   "id": "f4c4b9df09e187e4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Skeleton",
   "id": "f8bb23fbd7af88ce"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-17T08:51:27.119308Z",
     "start_time": "2024-08-17T08:51:27.106946Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_units_and_activation(num_classes):\n",
    "    \"\"\"\n",
    "        - One class : useless prediction\n",
    "        - Two classes : a binary choice for one of them (in or out)\n",
    "        - More classes : we have to give probabilities to belong to each class\n",
    "    :param num_classes:\n",
    "    :return: a tuple of (number of outputs, last activation function)\n",
    "    \"\"\"\n",
    "\n",
    "    if num_classes > 2:\n",
    "        return num_classes, 'softmax'\n",
    "    else:\n",
    "        return 1, 'sigmoid'"
   ],
   "id": "d3c897fb9dd68e2a",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def compile_classification_model(\n",
    "        model,\n",
    "        num_classes):\n",
    "    \"\"\"\n",
    "    Compile a model specifically for classification, using a loss fitted for the problem, and an Adam optimizer\n",
    "\n",
    "    :param model:\n",
    "    :param num_classes:\n",
    "    :return: an array of callbacks to plug when fitting, i.e. early stopping\n",
    "    \"\"\"\n",
    "\n",
    "    # We are in a classification problem, so we might use other losses (because our probabilities are either solids 1 or\n",
    "    # 0 in our dataset)\n",
    "    # If we only have two classes, our probability is straightforward : in or out => binary\n",
    "    loss = tf.keras.losses.BinaryCrossentropy()\n",
    "    if num_classes > 2:\n",
    "        loss = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "    optimizer = tf.keras.optimizers.Adam(model_params[\"learning_rate\"])\n",
    "\n",
    "    # Classification problem: we use accuracy as our metric\n",
    "    model.compile(loss=loss, optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "    callbacks = []\n",
    "    if model_params[\"early_stopping\"] is not None:\n",
    "        # Stop the training early if the validation loss doesn't decrease in 2 consecutive steps\n",
    "        callbacks.append(tf.keras.callbacks.EarlyStopping(monitor='loss', patience=model_params[\"early_stopping\"]))\n",
    "\n",
    "    return callbacks"
   ],
   "id": "512df3071a88dbd5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def fit_model_tensorboard(model,\n",
    "                          dataset=None,\n",
    "                          features=None,\n",
    "                          labels=None,\n",
    "                          callbacks=None):\n",
    "    \"\"\"\n",
    "    Fit the model using either:\n",
    "        - a given dataset\n",
    "        - separate features and labels\n",
    "    Each one of them will be split into training and validation sets\n",
    "    All the data will then be exported for TensorBoard analysis\n",
    "\n",
    "    :param callbacks: additional callbacks in addition to TensorBoard\n",
    "    :param labels:\n",
    "    :param features:\n",
    "    :param dataset: tf.data.Dataset object\n",
    "    :param model The Keras model to fit\n",
    "    \"\"\"\n",
    "\n",
    "    # Trick given by PEP to have immutable function arguments, and then fill with what we want\n",
    "    if callbacks is None:\n",
    "        callbacks = []\n",
    "\n",
    "    date = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "    log_dir = f'{model_params[\"log_dir\"]}/{model_params[\"name\"]}-{date}'\n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "    callbacks.append(tensorboard_callback)\n",
    "\n",
    "    kwargs = {\n",
    "        'epochs': model_params[\"epochs\"],\n",
    "        'verbose': model_params[\"verbose\"],\n",
    "\n",
    "        # 'batch_size': model_params[\"batch_size\"],\n",
    "        'callbacks': callbacks\n",
    "    }\n",
    "\n",
    "    if dataset is None:\n",
    "        kwargs['validation_split'] = model_params[\"validation_split\"]\n",
    "        model.fit(\n",
    "            tf.Variable(features),\n",
    "            labels,\n",
    "            **kwargs\n",
    "        )\n",
    "    else:\n",
    "        dataset_size = sum(1 for _ in dataset)\n",
    "        dataset = dataset.shuffle(buffer_size=1000)\n",
    "        validation_size = int(model_params[\"validation_split\"] * dataset_size)\n",
    "\n",
    "        validation_dataset = dataset.take(validation_size)\n",
    "        train_dataset = dataset.skip(validation_size)\n",
    "        kwargs['validation_data'] = validation_dataset\n",
    "        model.fit(train_dataset, **kwargs)\n",
    "\n",
    "    # Save model and parameters\n",
    "    model.save(f'models/{model_params[\"name\"]}-{date}.keras')\n",
    "    with open(f'models/{model_params[\"name\"]}-{date}.json', 'w') as file:\n",
    "        json.dump(model_params, file)\n",
    "    tf.keras.utils.plot_model(model, show_shapes=True)\n",
    "    print(f'Saved model to models/{model_params[\"name\"]}.keras')"
   ],
   "id": "dadedfd8988c4580"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Multi-layer perceptron (MLP)",
   "id": "3e417bbf51d7944a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def create_sequential(num_layers, units, activation, num_classes, input_shape, dropout_rate, normalization=None):\n",
    "    op_units, op_activation = get_units_and_activation(num_classes)\n",
    "\n",
    "    # Simpler to use: add layers as they are created\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.Input(shape=input_shape, sparse=True))\n",
    "    # model.add(tf.keras.layers.Dropout(rate=dropout_rate))\n",
    "    if normalization is not None:\n",
    "        model.add(normalization)\n",
    "\n",
    "    for i in range(num_layers):\n",
    "        model.add(tf.keras.layers.Dense(units=units, activation=activation))\n",
    "        # We should add a Dropout layer here to catch examples before they get to the output\n",
    "        # model.add(tf.keras.layers.Dropout(rate=dropout_rate))\n",
    "\n",
    "    model.add(tf.keras.layers.Dense(units=op_units, activation=op_activation))\n",
    "    return model"
   ],
   "id": "d1e72bfe9a05ff7e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def mlp_model(num_classes,\n",
    "              train_features,\n",
    "              train_labels,\n",
    "              layers=2,\n",
    "              epochs=1000,\n",
    "              learning_rate=1e-3):\n",
    "    # Input shape is a (n, m) matrix where :\n",
    "    # - n is the number of samples\n",
    "    # - m is the vocabulary size (so the number of probabilities)\n",
    "    # Hence the input shape of our model = the vocabulary size\n",
    "    model = create_sequential(layers, 64, 'relu', num_classes, train_features.shape[1:], 0.2)\n",
    "\n",
    "    callbacks = utils.compile_classification_model(model, num_classes)\n",
    "\n",
    "    utils.fit_model_tensorboard(model, features=train_features, labels=train_labels, callbacks=callbacks)"
   ],
   "id": "dbe893b4404b2a35"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### CNN",
   "id": "47a40f0871bc01bb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def get_cnn(num_blocks=3,\n",
    "            dropout_rate=0.2,\n",
    "            kernel_size=3,\n",
    "            num_filters=64,\n",
    "            pool_size=3):\n",
    "    \n",
    "    # ------------------\n",
    "    # STEP 1: tokenize our dataset to transform it into word sequences (those words are mapped to integers)\n",
    "    # ------------------\n",
    "    # This is done using the vectorization layer, which cannot be implemented here (only available to Functional API)\n",
    "    # Instead, we do the vectorization outside\n",
    "    \n",
    "    # The input shape will be deduced when the model is built, i.e., when we compile and evaluate the model\n",
    "    # for the first time (because it depends on the chosen sequence length)\n",
    "    inputs = tf.keras.Input(shape=(None,), dtype=\"int64\")\n",
    "    \n",
    "    # ------------------\n",
    "    # STEP 2: create the word embedding so the model can semantically interpret our sentences\n",
    "    # ------------------\n",
    "    num_features = len(vocabulary)\n",
    "    x = tf.keras.layers.Embedding(\n",
    "        num_features,\n",
    "        embedding_dim,\n",
    "        embeddings_initializer=tf.keras.initializers.Constant(embedding_matrix),\n",
    "        trainable=False,\n",
    "        # mask_zero=True,  # This option serves to ignore zeros in the padding (after having extended sequences to match\n",
    "                         # length) when passing to convolution blocks, or more efficiently to RNN blocks\n",
    "    )(inputs)\n",
    "    \n",
    "    for i in range(num_blocks):\n",
    "        x = tf.keras.layers.Dropout(rate=dropout_rate)(x)\n",
    "        x = tf.keras.layers.SeparableConv1D(\n",
    "            kernel_size=kernel_size,\n",
    "            filters=num_filters,\n",
    "            padding=\"same\",\n",
    "            activation='relu',\n",
    "            depthwise_initializer=tf.keras.initializers.RandomUniform,\n",
    "            bias_initializer=tf.keras.initializers.RandomUniform\n",
    "        )(x)\n",
    "        if i < num_blocks - 1:\n",
    "            x = tf.keras.layers.MaxPool1D(\n",
    "                pool_size=pool_size\n",
    "            )(x)\n",
    "    \n",
    "    x = tf.keras.layers.GlobalAveragePooling1D()(x)\n",
    "    x = tf.keras.layers.Dropout(rate=dropout_rate)(x)\n",
    "    \n",
    "    # Every model should have a dense layer at the end to return a coherent result\n",
    "    op_units, op_activation = get_units_and_activation(nb_classes)\n",
    "    outputs = tf.keras.layers.Dense(units=op_units, activation=op_activation)(x)\n",
    "    return tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "cnn_model = get_cnn()"
   ],
   "id": "2136adbf807dcc3f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def train_new_cnn_model(vocabulary,\n",
    "                        num_classes,\n",
    "                        train_texts,\n",
    "                        train_labels):\n",
    "\n",
    "    model = init_cnn_model(vocabulary, num_classes)\n",
    "    text_vectorizer = get_text_vectorizer(vocabulary)\n",
    "\n",
    "    # We create a unique dataset for our two components : features and labels\n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices((train_texts, train_labels))\n",
    "    # We then use a pre-processing pipeline which treatment will be optimized by TensorFlow\n",
    "    train_dataset = train_dataset.batch(model_params[\"batch_size\"]).map(lambda x, y: (text_vectorizer(x), y))\n",
    "\n",
    "    # If the model doesn't contain a preprocessing layer (so, using the Functional API) we must\n",
    "    # transform our dataset outside, and then pass it\n",
    "    callbacks = utils.compile_classification_model(model, num_classes)\n",
    "\n",
    "    utils.fit_model_tensorboard(model, dataset=train_dataset, callbacks=callbacks)\n",
    "    print(\"Model trained. Check statistics on TensorBoard using the logs/fit directory.\")\n",
    "    return model"
   ],
   "id": "f6e86f15a8c6c604"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Transformer encoder\n",
    "\n",
    "We don't transform a text into another, so we don't need a decoder"
   ],
   "id": "c8e8ace296ebd568"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-17T08:51:29.987222Z",
     "start_time": "2024-08-17T08:51:29.542075Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_transformer_encoder():\n",
    "    inputs = tf.keras.Input(shape=(None,), dtype=\"int64\")\n",
    "    x = tf.keras.layers.Embedding(\n",
    "        input_dim=num_features,\n",
    "        output_dim=embedding_dim,\n",
    "        embeddings_initializer=tf.keras.initializers.Constant(embedding_matrix),\n",
    "        trainable=False,  # Not to disrupt the already trained representation\n",
    "        # mask_zero=True # For more efficiency\n",
    "    )(inputs)\n",
    "    connection = x\n",
    "    x = tf.keras.layers.MultiHeadAttention(num_heads=2, key_dim=embedding_dim)(x, x)\n",
    "    x = tf.keras.layers.add((x, connection))\n",
    "    x = tf.keras.layers.LayerNormalization()(x)\n",
    "    \n",
    "    residual = x\n",
    "    x = tf.keras.layers.Dense(32, activation='relu')(x)\n",
    "    x = tf.keras.layers.Dense(embedding_dim)(x)  # Must have the same size as the residual connection\n",
    "    x = tf.keras.layers.add((x, residual))\n",
    "    x = tf.keras.layers.LayerNormalization()(x)\n",
    "    x = tf.keras.layers.GlobalMaxPooling1D()(x)\n",
    "    x = tf.keras.layers.Dropout(rate=0.5)(x)\n",
    "    \n",
    "    op_units, op_activation = get_units_and_activation(nb_classes)\n",
    "    outputs = tf.keras.layers.Dense(units=op_units, activation=op_activation)(x)\n",
    "    return tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "    \n",
    "tf.keras.config.disable_traceback_filtering()\n",
    "transformer_model = get_transformer_encoder()"
   ],
   "id": "4e7bffadad577d14",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-17T08:51:30.892955Z",
     "start_time": "2024-08-17T08:51:30.780271Z"
    }
   },
   "cell_type": "code",
   "source": [
    "transformer_model.compile(\n",
    "    optimizer=tf.keras.optimizers.RMSprop(),\n",
    "    loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "transformer_model.summary()"
   ],
   "id": "16e1df954c75d77f",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001B[1mModel: \"functional\"\u001B[0m\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001B[1m \u001B[0m\u001B[1mLayer (type)       \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1mOutput Shape     \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1m   Param #\u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1mConnected to     \u001B[0m\u001B[1m \u001B[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_1       │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;45mNone\u001B[0m)      │          \u001B[38;5;34m0\u001B[0m │ -                 │\n",
       "│ (\u001B[38;5;33mInputLayer\u001B[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ embedding_1         │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m300\u001B[0m) │  \u001B[38;5;34m6,000,000\u001B[0m │ input_layer_1[\u001B[38;5;34m0\u001B[0m]… │\n",
       "│ (\u001B[38;5;33mEmbedding\u001B[0m)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ multi_head_attenti… │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m300\u001B[0m) │    \u001B[38;5;34m722,100\u001B[0m │ embedding_1[\u001B[38;5;34m0\u001B[0m][\u001B[38;5;34m0\u001B[0m… │\n",
       "│ (\u001B[38;5;33mMultiHeadAttentio…\u001B[0m │                   │            │ embedding_1[\u001B[38;5;34m0\u001B[0m][\u001B[38;5;34m0\u001B[0m] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_2 (\u001B[38;5;33mAdd\u001B[0m)         │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m300\u001B[0m) │          \u001B[38;5;34m0\u001B[0m │ multi_head_atten… │\n",
       "│                     │                   │            │ embedding_1[\u001B[38;5;34m0\u001B[0m][\u001B[38;5;34m0\u001B[0m] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ layer_normalizatio… │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m300\u001B[0m) │        \u001B[38;5;34m600\u001B[0m │ add_2[\u001B[38;5;34m0\u001B[0m][\u001B[38;5;34m0\u001B[0m]       │\n",
       "│ (\u001B[38;5;33mLayerNormalizatio…\u001B[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_2 (\u001B[38;5;33mDense\u001B[0m)     │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m32\u001B[0m)  │      \u001B[38;5;34m9,632\u001B[0m │ layer_normalizat… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_3 (\u001B[38;5;33mDense\u001B[0m)     │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m300\u001B[0m) │      \u001B[38;5;34m9,900\u001B[0m │ dense_2[\u001B[38;5;34m0\u001B[0m][\u001B[38;5;34m0\u001B[0m]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_3 (\u001B[38;5;33mAdd\u001B[0m)         │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m300\u001B[0m) │          \u001B[38;5;34m0\u001B[0m │ dense_3[\u001B[38;5;34m0\u001B[0m][\u001B[38;5;34m0\u001B[0m],    │\n",
       "│                     │                   │            │ layer_normalizat… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ layer_normalizatio… │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m300\u001B[0m) │        \u001B[38;5;34m600\u001B[0m │ add_3[\u001B[38;5;34m0\u001B[0m][\u001B[38;5;34m0\u001B[0m]       │\n",
       "│ (\u001B[38;5;33mLayerNormalizatio…\u001B[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ global_max_pooling… │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m300\u001B[0m)       │          \u001B[38;5;34m0\u001B[0m │ layer_normalizat… │\n",
       "│ (\u001B[38;5;33mGlobalMaxPooling1…\u001B[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_3 (\u001B[38;5;33mDropout\u001B[0m) │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m300\u001B[0m)       │          \u001B[38;5;34m0\u001B[0m │ global_max_pooli… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_4 (\u001B[38;5;33mDense\u001B[0m)     │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m1\u001B[0m)         │        \u001B[38;5;34m301\u001B[0m │ dropout_3[\u001B[38;5;34m0\u001B[0m][\u001B[38;5;34m0\u001B[0m]   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_1       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ embedding_1         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>) │  <span style=\"color: #00af00; text-decoration-color: #00af00\">6,000,000</span> │ input_layer_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ multi_head_attenti… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>) │    <span style=\"color: #00af00; text-decoration-color: #00af00\">722,100</span> │ embedding_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttentio…</span> │                   │            │ embedding_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>) │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ multi_head_atten… │\n",
       "│                     │                   │            │ embedding_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ layer_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>) │        <span style=\"color: #00af00; text-decoration-color: #00af00\">600</span> │ add_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)  │      <span style=\"color: #00af00; text-decoration-color: #00af00\">9,632</span> │ layer_normalizat… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>) │      <span style=\"color: #00af00; text-decoration-color: #00af00\">9,900</span> │ dense_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>) │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],    │\n",
       "│                     │                   │            │ layer_normalizat… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ layer_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>) │        <span style=\"color: #00af00; text-decoration-color: #00af00\">600</span> │ add_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ global_max_pooling… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ layer_normalizat… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalMaxPooling1…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ global_max_pooli… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">301</span> │ dropout_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Total params: \u001B[0m\u001B[38;5;34m6,743,133\u001B[0m (25.72 MB)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">6,743,133</span> (25.72 MB)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Trainable params: \u001B[0m\u001B[38;5;34m743,133\u001B[0m (2.83 MB)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">743,133</span> (2.83 MB)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Non-trainable params: \u001B[0m\u001B[38;5;34m6,000,000\u001B[0m (22.89 MB)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">6,000,000</span> (22.89 MB)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-17T09:23:55.213441Z",
     "start_time": "2024-08-17T08:51:33.558858Z"
    }
   },
   "cell_type": "code",
   "source": [
    "date = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "transformer_model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=validation_dataset,\n",
    "    epochs=20,\n",
    "    callbacks=[\n",
    "        tf.keras.callbacks.ModelCheckpoint(f'models/transformer_best.keras', save_best_only=True, monitor='val_loss'),\n",
    "        tf.keras.callbacks.ModelCheckpoint('models/transformer{epoch:02d}-{val_loss:.2f}.keras'),\n",
    "        tf.keras.callbacks.BackupAndRestore(backup_dir=f'/tmp/backup/transformer--{date}'),\n",
    "        tf.keras.callbacks.TensorBoard(log_dir=f'logs/fit/transformer--{date}', histogram_freq=1)\n",
    "    ]\n",
    ")"
   ],
   "id": "4840d0b19ad46a22",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1723884707.985003   36410 service.cc:146] XLA service 0x7f5a7002adf0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1723884707.985048   36410 service.cc:154]   StreamExecutor device (0): NVIDIA GeForce GTX 1050, Compute Capability 6.1\n",
      "2024-08-17 10:51:48.034539: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-08-17 10:51:48.515373: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:531] Loaded cuDNN version 8907\n",
      "I0000 00:00:1723884714.429655   36410 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m626/626\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 129ms/step - accuracy: 0.5092 - loss: 0.8531"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-17 10:53:36.436121: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 1440000000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m626/626\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m112s\u001B[0m 165ms/step - accuracy: 0.5092 - loss: 0.8529 - val_accuracy: 0.5551 - val_loss: 0.6908\n",
      "Epoch 2/20\n",
      "\u001B[1m625/626\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 126ms/step - accuracy: 0.5199 - loss: 0.6990"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-17 10:55:18.014340: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 1440000000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m626/626\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m101s\u001B[0m 161ms/step - accuracy: 0.5200 - loss: 0.6990 - val_accuracy: 0.6478 - val_loss: 0.6520\n",
      "Epoch 3/20\n",
      "\u001B[1m625/626\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 122ms/step - accuracy: 0.5998 - loss: 0.6606"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-17 10:56:57.409359: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 1440000000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m626/626\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m99s\u001B[0m 158ms/step - accuracy: 0.5999 - loss: 0.6605 - val_accuracy: 0.7282 - val_loss: 0.5372\n",
      "Epoch 4/20\n",
      "\u001B[1m625/626\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 121ms/step - accuracy: 0.7246 - loss: 0.5346"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-17 10:58:35.104573: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 1440000000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m626/626\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m98s\u001B[0m 156ms/step - accuracy: 0.7246 - loss: 0.5346 - val_accuracy: 0.7851 - val_loss: 0.4556\n",
      "Epoch 5/20\n",
      "\u001B[1m625/626\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 123ms/step - accuracy: 0.7681 - loss: 0.4746"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-17 11:00:15.726377: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 1440000000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m626/626\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m101s\u001B[0m 161ms/step - accuracy: 0.7681 - loss: 0.4746 - val_accuracy: 0.8017 - val_loss: 0.4257\n",
      "Epoch 6/20\n",
      "\u001B[1m626/626\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m84s\u001B[0m 133ms/step - accuracy: 0.7940 - loss: 0.4380 - val_accuracy: 0.7802 - val_loss: 0.4503\n",
      "Epoch 7/20\n",
      "\u001B[1m626/626\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m101s\u001B[0m 160ms/step - accuracy: 0.8064 - loss: 0.4190 - val_accuracy: 0.8079 - val_loss: 0.4125\n",
      "Epoch 8/20\n",
      "\u001B[1m626/626\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m84s\u001B[0m 133ms/step - accuracy: 0.8143 - loss: 0.3995 - val_accuracy: 0.7893 - val_loss: 0.4353\n",
      "Epoch 9/20\n",
      "\u001B[1m626/626\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m85s\u001B[0m 136ms/step - accuracy: 0.8300 - loss: 0.3798 - val_accuracy: 0.8081 - val_loss: 0.4155\n",
      "Epoch 10/20\n",
      "\u001B[1m626/626\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m102s\u001B[0m 162ms/step - accuracy: 0.8361 - loss: 0.3699 - val_accuracy: 0.8175 - val_loss: 0.3989\n",
      "Epoch 11/20\n",
      "\u001B[1m626/626\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m85s\u001B[0m 135ms/step - accuracy: 0.8438 - loss: 0.3481 - val_accuracy: 0.8119 - val_loss: 0.4228\n",
      "Epoch 12/20\n",
      "\u001B[1m626/626\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m88s\u001B[0m 141ms/step - accuracy: 0.8521 - loss: 0.3394 - val_accuracy: 0.8105 - val_loss: 0.4129\n",
      "Epoch 13/20\n",
      "\u001B[1m626/626\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m92s\u001B[0m 146ms/step - accuracy: 0.8586 - loss: 0.3227 - val_accuracy: 0.8099 - val_loss: 0.4506\n",
      "Epoch 14/20\n",
      "\u001B[1m626/626\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m92s\u001B[0m 146ms/step - accuracy: 0.8673 - loss: 0.3089 - val_accuracy: 0.8031 - val_loss: 0.4611\n",
      "Epoch 15/20\n",
      "\u001B[1m626/626\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m93s\u001B[0m 148ms/step - accuracy: 0.8752 - loss: 0.2887 - val_accuracy: 0.8061 - val_loss: 0.4687\n",
      "Epoch 16/20\n",
      "\u001B[1m626/626\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m93s\u001B[0m 148ms/step - accuracy: 0.8815 - loss: 0.2775 - val_accuracy: 0.7971 - val_loss: 0.5010\n",
      "Epoch 17/20\n",
      "\u001B[1m626/626\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m95s\u001B[0m 151ms/step - accuracy: 0.8896 - loss: 0.2641 - val_accuracy: 0.8075 - val_loss: 0.5002\n",
      "Epoch 18/20\n",
      "\u001B[1m626/626\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m93s\u001B[0m 148ms/step - accuracy: 0.8937 - loss: 0.2449 - val_accuracy: 0.8057 - val_loss: 0.5352\n",
      "Epoch 19/20\n",
      "\u001B[1m626/626\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m93s\u001B[0m 148ms/step - accuracy: 0.9019 - loss: 0.2344 - val_accuracy: 0.8065 - val_loss: 0.5588\n",
      "Epoch 20/20\n",
      "\u001B[1m626/626\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m139s\u001B[0m 144ms/step - accuracy: 0.9055 - loss: 0.2178 - val_accuracy: 0.8023 - val_loss: 0.5700\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7f5b1a17a680>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-17T09:39:55.716041Z",
     "start_time": "2024-08-17T09:39:07.950698Z"
    }
   },
   "cell_type": "code",
   "source": [
    "best_transformer = tf.keras.models.load_model('models/transformer.keras')\n",
    "best_transformer.evaluate(test_dataset)"
   ],
   "id": "46d021313d737af1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m782/782\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m35s\u001B[0m 43ms/step - accuracy: 0.7949 - loss: 0.4414\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.4108327627182007, 0.8143600225448608]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 14
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
